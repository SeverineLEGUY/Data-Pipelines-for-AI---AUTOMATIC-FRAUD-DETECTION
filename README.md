
# ðŸš€ DÃ©ploiement d'Airflow : une approche hybride

## ðŸŽ¯ Objectif

Ce guide explique comment dÃ©ployer **Airflow** avec **Docker Compose**, et configurer les connexions Ã  **PostgreSQL** et **AWS S3** directement dans lâ€™interface Airflow pour dÃ©ployer votre premiÃ¨re vraie pipeline !

---

## ðŸ›  PrÃ©requis

- **Docker & Docker Compose** installÃ©s
- Un **bucket S3** 
- Une **base de donnÃ©es PostgreSQL** 

---


## ðŸ“Œ 1. docker-compose.yaml (DÃ©ploiement Airflow)

RÃ©cuperez le docker-compose 

---

## ðŸ“Œ 2. DÃ©marrage du Serveur Airflow

1. Lancer les conteneurs : (pas besoin de build c'est fait dans le docker compose)

```bash
docker-compose up airflow-init
```

```bash
docker-compose up
```

2. AccÃ©der Ã  Airflow :
   - Ouvrez http://localhost:8080.
   - Connectez-vous avec airflow / airflow.

---

## ðŸ“Œ 3. Configuration des Connexions dans Airflow

### Connexion AWS (S3)

1. Admin > Connections dans Airflow.
2. CrÃ©ez une connexion avec :

   - Conn Id : aws_default
   - Conn Type : Amazon Web Services
   - AWS Access Key ID : VOTRE_ACCESS_KEY
   - AWS Secret Access Key : VOTRE_SECRET_KEY
   - Extra :

   ```json
   {
     "region_name": "VOTRE_REGION"
   }
   ```

3. Sauvegardez.

### Connexion PostgreSQL

1. Admin > Connections dans Airflow.
2. CrÃ©ez une connexion avec :
   - Conn Id : postgres_default
   - Conn Type : Postgres
   - Host : VOTRE_HOST
   - Database : VOTRE_BDD
   - Login : VOTRE_UTILISATEUR
   - Password : VOTRE_MOT_DE_PASSE
   - Port : 5432
   - Extra :
   ```json
   {
     "sslmode": "require"
   }
   ```
3. Sauvegardez.

---

## ðŸ“Œ 4. Vous pouvez maintenant trigger votre dags !
